# Music Generation with RNNs

**Assignment 3** - Einführung in Deep Learning (WiSe 25/26)  
**Student:** Md Amanullah  
**Matrikelnummer:** 5466324

---

## Overview

This project implements a music generation system using Recurrent Neural Networks (RNNs). The model is trained on the IrishMAN dataset containing 214,122 Irish folk tunes in ABC notation format. It learns to generate new music by predicting the next character in a sequence, similar to how language models work but applied to music.

The system successfully generates syntactically valid ABC notation tunes that follow musical conventions learned from the training data.

---

## Quick Results

| Metric | Value |
|--------|-------|
| Top-1 Accuracy | 74.91% |
| Top-5 Accuracy | 95.40% |
| Validation Loss | 0.7729 |
| Training Loss | 0.7918 |
| Total Epochs | 10 |
| Model Parameters | 959,715 |

---

## Project Structure

```
.
├── submission_onepage.pdf              # Main deliverable (one-page summary)
├── uebung3_complete_SUBMISSION.ipynb   # Complete implementation code
├── best_model.pt                       # Trained model weights
├── generated_music.abc                 # Generated music example
├── training_curves.png                 # Training visualization
├── abc_structure_visualization.png     # Data analysis plots
├── uebungsblatt_03.pdf                 # Assignment specification
├── events.out.tfevents                 # TensorBoard logs
└── README.md                           # This file
```

---

## File Descriptions

### Main Deliverable
- **submission_onepage.pdf** (187 KB)  
  One-page professional summary with approach, results, generated music example, challenges, and solutions. This is the main assignment deliverable.

### Implementation
- **uebung3_complete_SUBMISSION.ipynb** (522 KB)  
  Complete Jupyter notebook with all code including data loading, preprocessing, model architecture, training functions, evaluation, and music generation. Fully documented and commented.

### Model & Data
- **best_model.pt** (3.8 MB)  
  Trained model weights saved after 10 epochs. Load using `torch.load()` to generate music without retraining.

- **generated_music.abc** (0.2 KB)  
  Example of music generated by the trained model in ABC notation format.

### Visualizations
- **training_curves.png** (157 KB)  
  Training progress visualization showing loss curves and accuracy metrics across all epochs.

- **abc_structure_visualization.png** (80 KB)  
  Data analysis plots showing header distribution, note distribution, line length distribution, and cumulative note count.

### Reference
- **uebungsblatt_03.pdf** (48 KB)  
  Original assignment specification document.

- **events.out.tfevents** (75 KB)  
  TensorBoard event file with detailed training metrics.

---

## Model Architecture

```
Input (ABC notation)
    ↓
Embedding Layer (128D)
    ↓
2-Layer LSTM (256 hidden units each)
    ↓
Dropout (0.3)
    ↓
Output Layer (99 vocabulary)
    ↓
Generated Music (ABC notation)
```

### Architecture Details

| Component | Configuration |
|-----------|----------------|
| Type | 2-Layer LSTM |
| Embedding Dimension | 128 |
| Hidden Units | 256 per layer |
| Dropout Rate | 0.3 |
| Vocabulary Size | 99 tokens |
| Total Parameters | 959,715 |

---

## Training Configuration

| Parameter | Value |
|-----------|-------|
| Optimizer | Adam |
| Learning Rate | 0.001 |
| Batch Size | 32 |
| Gradient Clipping | max norm 1.0 |
| Loss Function | CrossEntropyLoss |
| LR Scheduler | ReduceLROnPlateau |
| Training Device | CUDA (GPU) |
| Training Time | ~90 minutes |
| Total Epochs | 10 |

---

## Dataset

| Aspect | Value |
|--------|-------|
| Training Samples | 214,122 tunes |
| Validation Samples | 2,162 tunes |
| Format | ABC notation |
| Vocabulary Size | 99 unique characters |
| Sequence Length | 100-1000+ characters |

---

## Data Preprocessing

1. **Vocabulary Extraction**  
   Extracted 99 unique characters from ABC notation files.

2. **Encoding**  
   Each tune encoded as sequence of token indices.

3. **Sequence Pairs**  
   Created input-target pairs where input is all tokens except last, target is all tokens except first (shifted by 1).

4. **Dynamic Padding**  
   Implemented dynamic padding within batches to handle variable-length sequences (100-1000+ characters) efficiently without wasting memory.

---

## Generated Music Example

```
X:1
T:Generated Tune
M:4/4
L:1/8
K:G
B2 | c3 A F2 D2 | G3 A B2 AG | FGAB cAFA |
G2 D2 D2 GA | B3 A G2 A2 | FAce d3 e |
fgfe d2 c2 | B4 B2 d2 | cBAB ABAG |
FG A2 A2 d2 | cBAG F2 G2 | A2 F2 G4 |]
```

The generated tune demonstrates valid ABC notation syntax with proper headers (X:, T:, M:, L:, K:), logical note sequences, and realistic musical patterns.

---

## Challenges & Solutions

### 1. GPU Training Power Constraints
**Challenge:** Training 214K samples for 10 epochs required significant GPU memory and ~90 minutes of computation.  
**Solution:** Optimized batch processing with dynamic padding, efficient data loading, and gradient accumulation strategies.

### 2. Gradient Instability
**Challenge:** RNNs suffer from vanishing/exploding gradients with long sequences.  
**Solution:** Applied gradient clipping (max norm 1.0) and used LSTM cells for better stability.

### 3. Variable-Length Sequences
**Challenge:** ABC notation files range from 100-1000+ characters.  
**Solution:** Implemented dynamic padding in batches to handle variable lengths efficiently.

### 4. Overfitting Risk
**Challenge:** Large model on sequential data prone to overfitting.  
**Solution:** Applied dropout (0.3), learning rate scheduling (ReduceLROnPlateau), and validation monitoring.

### 5. Large Vocabulary
**Challenge:** 99 tokens is substantial for character-level modeling.  
**Solution:** Used embedding layer (128D) to learn meaningful token representations.

---

## Results Analysis

**Top-5 Accuracy (95.40%)**  
The correct token is usually in the top 5 predictions even when exact next-token prediction is ambiguous. This indicates strong musical pattern learning.

**Top-1 Accuracy (74.91%)**  
The model correctly predicts the exact next character about 75% of the time, showing effective modeling of ABC notation structure.

**Training Convergence**  
Consistent improvement across all 10 epochs without significant overfitting, indicating good generalization to unseen musical sequences.

---

## How to Use

### Prerequisites
```bash
pip install torch datasets tensorboard music21 tqdm numpy
```

### Training
Open and run `uebung3_complete_SUBMISSION.ipynb` in Jupyter Notebook

### Generating Music
Load the trained model and use the generation function with temperature parameter to control creativity:
```python
model = torch.load('best_model.pt')
generated_music = generate_music(model, length=200, temperature=0.8)
```

---

## Key Insights

- RNNs effectively learn sequential patterns in music
- LSTM cells handle long sequences better than vanilla RNNs
- Character-level modeling works well for structured formats like ABC notation
- GPU acceleration is essential for large-scale training
- Proper regularization prevents overfitting while maintaining generalization

---

## Conclusion

This project successfully demonstrates music generation using RNNs. The model achieved strong accuracy metrics (95.40% Top-5) and generates syntactically valid ABC notation tunes that follow musical conventions. The main challenge was managing GPU resources for training on a large dataset, which was addressed through optimization techniques. The results show that RNNs are effective for sequence generation tasks in music.

---

## References

- **Dataset:** IrishMAN (Irish Music Annotation Network) - [Hugging Face](https://huggingface.co/datasets/sander-wood/irishman)
- **Framework:** PyTorch
- **Music Format:** ABC Notation - [https://abc.rectanglered.com/](https://abc.rectanglered.com/)

---

**Last Updated:** January 19, 2026
#   m u s i c - r n n  
 